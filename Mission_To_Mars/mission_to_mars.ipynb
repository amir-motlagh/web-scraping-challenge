{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the relevant libraries you may use\n",
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import time\n",
    "import datetime as dt\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Mars Data\n",
    "\n",
    "## NASA Mars News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will be scraping information from the NASA website regarding Mars\n",
    "# Define the the URL\n",
    "url = 'https://mars.nasa.gov/news'\n",
    "\n",
    "# Retrive the page with the requests module\n",
    "response = requests.get(url)\n",
    "# Convert the response to text to obtain the html\n",
    "html = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish chrome driver executable path. Make sure to define actual location on your drive.\n",
    "executable_path ={'executable_path': 'C:/Users/NAVID/chromedriver.exe'}\n",
    "# Open a splinter browser\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visit the defined URL on your splinter broswers\n",
    "browser.visit(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a BeautifulSoup object with the splinter broswer.html object and parse the html with 'html.parser' or 'lxml'\n",
    "soup = bs(browser.html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape the first instance of latest news title text and assign to a variable\n",
    "# Find the first article\n",
    "first_news_article = soup.find('li', class_=\"slide\")\n",
    "\n",
    "# Find the title within that article summary and convert into .text or .get_text() and then .strip() of '/n'\n",
    "news_title = first_news_article.find('div', class_='content_title').text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NASA to Provide Update on InSight Mars Lander'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://mars.nasa.gov/news/news/9188/nasa-to-provide-update-on-insight-mars-lander/'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the article link url\n",
    "article_link_string = first_news_article.find('a')['href']\n",
    "article_url = url + article_link_string\n",
    "article_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape the first instance of latest paragraph text and assign to a variable\n",
    "# Find the paragraph within that article summary and convert into .text and then .strip() of '/n'\n",
    "news_p = first_news_article.find('div', class_=\"article_teaser_body\").text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NASA and InSight leaders will share the latest on the pioneering spacecraftâ€™s science findings and discuss future milestones for the mission.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JPL Mars Space Images - Featured Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<splinter.element_list.ElementList at 0x1901aa3f340>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Open a splinter browser to scrape the desired images\n",
    "# Define the URL path\n",
    "url_2 = 'https://spaceimages-mars.com'\n",
    "\n",
    "# Using the already established splinter engine, open the url in broswer\n",
    "# Visit the defined URL on your splinter broswers\n",
    "browser.visit(url_2)\n",
    "\n",
    "# delay action until browser loads\n",
    "time.sleep(10)\n",
    "\n",
    "# click on the sprinter browser link 'FULL IMAGE' to see the image we want to store\n",
    "browser.links.find_by_partial_text('FULL IMAGE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and click the full image button\n",
    "full_image_elem = browser.find_by_tag('button')[1]\n",
    "full_image_elem.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soupify the browser html\n",
    "html = browser.html\n",
    "img_soup = bs(html, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'image/featured/mars2.jpg'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Locate the 'div' and class attribute where the image is found and \n",
    "img_url_rel = img_soup.find('img', class_='fancybox-image').get('src')\n",
    "img_url_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://spaceimages-mars.com/image/featured/mars2.jpg'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the base url to create an absolute url\n",
    "img_url = f'https://spaceimages-mars.com/{img_url_rel}'\n",
    "img_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mars Facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use Pandas to scrape the table information from the space-fact.com website on Mars\n",
    "# Define the url\n",
    "url = 'https://space-facts.com/mars/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pd.read_html() will pull a list dataframes of all the tables\n",
    "tables = pd.read_html(url)\n",
    "tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to slice off the 1st table from the list\n",
    "mars_facts_tbl = tables[0]\n",
    "mars_facts_tbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mars_facts_tbl.columns = ['Attribute','Values']\n",
    "mars_facts_tbl\n",
    "\n",
    "# Set the index to the Atrributes column\n",
    "mars_facts_tbl.set_index('Attribute', inplace=True)\n",
    "mars_facts_tbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_mars_tbl = mars_facts_tbl.to_html()\n",
    "html_mars_tbl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mars Hemispheres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visit URL\n",
    "url_3 = 'https://marshemispheres.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# browse the url\n",
    "# Visit the defined URL on your splinter broswers\n",
    "browser.visit(url_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soupify \n",
    "html = browser.html\n",
    "soup = bs(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store dictonary values for the keys of 'image_url' and 'title'\n",
    "hemisphere_image_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use .find_all() to slice out the html we will loop through to visit different webpages and scrape the data\n",
    "image_links = soup.find_all('div', class_='item')\n",
    "image_links\n",
    "# browser.find_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in image_links:\n",
    "    \n",
    "    # Find the url link string from the 'a' tag and call the 'href' string\n",
    "    link = item.find('a')['href']\n",
    "    \n",
    "    # Combine the root url from above and the link url\n",
    "    url_4 = url_3 + link\n",
    "    \n",
    "    # Open the splinter browser using the url_4 link we just created\n",
    "    browser.visit(url_4)\n",
    "\n",
    "    # Let the browser load for 1 seconds before scraping data\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # Soupify the page\n",
    "    soup = bs(browser.html, 'html.parser')\n",
    "    \n",
    "    # Find the link to the image in the 'ul' tag, then the 'a' tag, and then call the 2nd item 'href'\n",
    "    # Store link string in variable 'image_link_hemi'\n",
    "    image_link_hemi = soup.find('ul').find_all('a')[0]['href']\n",
    "    \n",
    "    # Find the title name using the 'h2' tag and class attribute 'title', and then pull the .text\n",
    "    # Store title in variable 'title_text'\n",
    "    title_text = soup.find('h2', class_='title').text\n",
    "    \n",
    "    # Append the hemisphere list with a dictionary of the keys and values\n",
    "    hemisphere_image_list.append({\n",
    "        'title': title_text, \n",
    "        'img_url': image_link_hemi\n",
    "    })\n",
    "    \n",
    "    # Print out success message\n",
    "    print(f'Scrape of {title_text} COMPLETE')\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if data is all there\n",
    "hemisphere_image_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of all the web scraped data\n",
    "dict_mars_scrape = {\n",
    "    'news_title': news_title,\n",
    "    'news_p': news_p,\n",
    "    'article_url': article_url,\n",
    "    'featured_image_url': img_url,\n",
    "    'html_mars_tbl': html_mars_tbl,\n",
    "    'hemisphere_image_list': hemisphere_image_list,\n",
    "    # Add the time of the scrape to the dictionary\n",
    "    'scrape_time': dt.datetime.now()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_mars_scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "161dff17c271741c9720b06829c3526a6ba9e52280bfbaaa41922ae645c89b4d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
